{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"../data/train_data.csv\")\n",
    "\n",
    "data[\"labels\"] = data[\"rating\"] - 1\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_data(data, max_len=512):\n",
    "    return tokenizer(data[\"review\"].tolist(), max_length=max_len, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Tokenizacja zbiorów danych\n",
    "train_encodings = tokenize_data(train_data)\n",
    "val_encodings = tokenize_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "def create_data_loader(encodings, labels):\n",
    "    dataset = TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], torch.tensor(labels))\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "train_loader = create_data_loader(train_encodings, train_data[\"labels\"])\n",
    "val_loader = create_data_loader(val_encodings, val_data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(data[\"rating\"].unique()))\n",
    "\n",
    "# Definicja urządzenia (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Funkcje do trenowania i ewaluacji\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    # Implementacja procedury treningowej\n",
    "    pass\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    # Implementacja procedury ewaluacji\n",
    "    pass\n",
    "\n",
    "\n",
    "# Procedura trenowania\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    eval_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../data/test_data.csv\")\n",
    "\n",
    "test_encodings = tokenize_data(test_data)\n",
    "test_loader = create_data_loader(test_encodings, test_data[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for batch in test_loader:\n",
    "    preds = model(batch[0].to(device), batch[1].to(device))\n",
    "    predictions.extend(preds.argmax(dim=1).tolist())\n",
    "\n",
    "pd.DataFrame({\"Predicted Rating\": predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
